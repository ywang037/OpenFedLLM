{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class FedTrainConfig:\n",
    "    # Training parameters\n",
    "    max_steps = 10\n",
    "    num_rounds = 100\n",
    "    batch_size = 8\n",
    "    gradient_accumulation_steps = 1\n",
    "    seq_length = 512\n",
    "    learning_rate = 5e-5\n",
    "    \n",
    "    # Federated learning parameters\n",
    "    num_clients = 20\n",
    "    sample_clients = 2\n",
    "    fed_alg = \"fedavg\"\n",
    "    \n",
    "    # Model parameters\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32  # 通常设置为lora_r的2倍\n",
    "    \n",
    "    # Dataset parameters\n",
    "    dataset_name = \"vicgalle/alpaca-gpt4\"\n",
    "    dataset_sample = 20000\n",
    "    template = \"alpaca\"\n",
    "    \n",
    "    # Device configuration\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    load_in_8bit = True\n",
    "    \n",
    "    # Output configuration\n",
    "    output_dir = \"./output\"\n",
    "    \n",
    "config = FedTrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create widgets for hyperparameters\n",
    "lr_slider = widgets.FloatLogSlider(\n",
    "    value=5e-5,\n",
    "    min=-6, max=-3,\n",
    "    step=0.5,\n",
    "    description='learning rate'\n",
    ")\n",
    "\n",
    "batch_selector = widgets.Dropdown(\n",
    "    options=[4, 8, 16, 32],\n",
    "    value=8,\n",
    "    description='batch size'\n",
    ")\n",
    "\n",
    "round_slider = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=10, max=500,\n",
    "    step=10,\n",
    "    description='num rounds'\n",
    ")\n",
    "\n",
    "display(lr_slider, batch_selector, round_slider)\n",
    "\n",
    "# Update config with new hyperparameters\n",
    "config.learning_rate = lr_slider.value\n",
    "config.batch_size = batch_selector.value\n",
    "config.num_rounds = round_slider.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config, save_config\n",
    "\n",
    "# Initialize configuration object\n",
    "script_args, fed_args, peft_config = get_config()\n",
    "script_args.dataset_name = config.dataset_name\n",
    "script_args.dataset_sample = config.dataset_sample\n",
    "script_args.model_name_or_path = config.model_name\n",
    "script_args.output_dir = config.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 00:33:07,913] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b1713e1b2b418bb75f15ee65120e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165\n",
      "Model is loaded to cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "\n",
    "# Initialize model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    load_in_8bit=config.load_in_8bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare model for 8-bit training\n",
    "if config.load_in_8bit:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Model is loaded to {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang_yuan/miniforge3/envs/fedllm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                                            Output Shape              Param #\n",
       "===================================================================================================================\n",
       "PeftModelForCausalLM                                              [1, 32, 7, 128]           --\n",
       "├─LoraModel: 1-1                                                  [1, 32, 7, 128]           --\n",
       "│    └─LlamaForCausalLM: 2-1                                      --                        --\n",
       "│    │    └─LlamaModel: 3-1                                       [1, 32, 7, 128]           --\n",
       "│    │    │    └─Embedding: 4-1                                   [1, 7, 4096]              (131,072,000)\n",
       "│    │    │    └─ModuleList: 4-2                                  --                        --\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-1                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-1                      [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-2                    [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-3                      [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-4                          [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-2                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-5                      [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-6                    [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-7                      [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-8                          [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-3                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-9                      [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-10                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-11                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-12                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-4                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-13                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-14                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-15                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-16                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-5                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-17                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-18                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-19                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-20                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-6                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-21                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-22                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-23                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-24                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-7                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-25                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-26                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-27                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-28                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-8                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-29                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-30                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-31                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-32                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-9                      [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-33                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-34                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-35                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-36                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-10                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-37                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-38                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-39                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-40                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-11                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-41                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-42                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-43                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-44                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-12                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-45                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-46                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-47                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-48                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-13                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-49                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-50                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-51                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-52                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-14                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-53                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-54                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-55                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-56                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-15                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-57                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-58                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-59                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-60                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-16                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-61                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-62                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-63                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-64                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-17                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-65                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-66                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-67                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-68                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-18                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-69                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-70                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-71                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-72                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-19                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-73                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-74                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-75                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-76                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-20                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-77                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-78                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-79                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-80                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-21                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-81                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-82                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-83                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-84                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-22                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-85                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-86                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-87                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-88                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-23                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-89                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-90                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-91                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-92                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-24                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-93                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-94                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-95                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-96                         [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-25                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-97                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-98                   [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-99                     [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-100                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-26                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-101                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-102                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-103                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-104                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-27                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-105                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-106                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-107                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-108                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-28                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-109                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-110                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-111                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-112                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-29                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-113                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-114                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-115                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-116                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-30                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-117                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-118                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-119                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-120                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-31                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-121                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-122                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-123                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-124                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    │    └─LlamaDecoderLayer: 5-32                     [1, 7, 4096]              --\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-125                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaAttention: 6-126                  [1, 7, 4096]              67,371,008\n",
       "│    │    │    │    │    └─LlamaRMSNorm: 6-127                    [1, 7, 4096]              (4,096)\n",
       "│    │    │    │    │    └─LlamaMLP: 6-128                        [1, 7, 4096]              (135,266,304)\n",
       "│    │    │    └─LlamaRMSNorm: 4-3                                [1, 7, 4096]              (4,096)\n",
       "│    │    └─Linear: 3-2                                           [1, 7, 32000]             (131,072,000)\n",
       "===================================================================================================================\n",
       "Total params: 6,746,804,224\n",
       "Trainable params: 8,388,608\n",
       "Non-trainable params: 6,738,415,616\n",
       "Total mult-adds (G): 5.67\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 93.14\n",
       "Params size (MB): 6485.46\n",
       "Estimated Total Size (MB): 6578.60\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "# Create simple input\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "sample_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(config.device)\n",
    "\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=inputs[\"input_ids\"],\n",
    "    depth=6,  # Low depth to start - increase for more details\n",
    "    dtypes=[torch.bfloat16]\n",
    "    # col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    # device=config.device,\n",
    "    # verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Memory Footprint: 7.10 GB\n"
     ]
    }
   ],
   "source": [
    "gb_footprint = model.get_memory_footprint()/ (1024 * 1024 * 1024)\n",
    "print(f\"Model Memory Footprint: {gb_footprint:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_dataset, process_sft_dataset\n",
    "from federated_learning import split_dataset\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = get_dataset(script_args.dataset_name)\n",
    "dataset = process_sft_dataset(script_args.dataset_name, dataset, config.dataset_sample)\n",
    "\n",
    "# Split client data\n",
    "print(\"Spliting client data...\")\n",
    "local_datasets = split_dataset(fed_args, script_args, dataset)\n",
    "sample_num_list = [len(local_datasets[i]) for i in range(fed_args.num_clients)]\n",
    "clear_output()\n",
    "print(\"Dataset loaded and split successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
